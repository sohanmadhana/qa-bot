{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a69619-e88d-48a0-94f9-3bf5a2c50537",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e20e8-c159-4137-afbc-76f79192ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head documents.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210b54b-9349-42f6-a261-8d0e594abc76",
   "metadata": {},
   "source": [
    "<b>Load the documents</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f8c8c7-96ec-4372-9967-8db5d31fc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./documents.json', 'rt') as f_in:\n",
    "    documents_file = json.load(f_in)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_file:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831717a8-1ef1-4aca-8371-25abfcf0e55a",
   "metadata": {},
   "source": [
    "<b>Connecting and Loading ElasticSearch </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29538d8-07c7-4fc6-824d-d2c07a6c4f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '3689fda5931c', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'K93Hg-FETy6DsG2obzSrbA', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc79afe-1588-42b7-9f0e-aa8271637c82",
   "metadata": {},
   "source": [
    "<b>Create indexes for the document </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2abbb0-6f3b-4dd1-a342-2cba9170d4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "response = es.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1895b8d2-25a5-4dda-be91-21c9deacb24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/share/virtualenvs/qa-bot-da_7AUzJ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|████████████████████████████████████████████████████████████████████████| 948/948 [00:22<00:00, 41.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    es.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab1e88b-b374-4b40-8240-e1393ec359c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How do I join the course after it has started?\"\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": user_question,\n",
    "                    \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                    \"type\": \"best_fields\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"course\": \"data-engineering-zoomcamp\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2778f6-bec6-4691-8dd9-4952a2d12f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: General course-related questions\n",
      "Question: Course - Can I still join the course after the start date?\n",
      "Answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: Course - Can I follow the course after it finishes?\n",
      "Answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: Course - What can I do before the course starts?\n",
      "Answer: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaconda)\n",
      "Terraform\n",
      "Git\n",
      "Look over the prerequisites and syllabus to see if you are comfortable with these subjects.\n",
      "\n",
      "\n",
      "Section: General course-related questions\n",
      "Question: How do I use Git / GitHub for this course?\n",
      "Answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "\n",
      "Section: Workshop 1 - dlthub\n",
      "Question: How do I install the necessary dependencies to run the code?\n",
      "Answer: Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    doc = hit['_source']\n",
    "    print(f\"Section: {doc['section']}\\nQuestion: {doc['question']}\\nAnswer: {doc['text']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbba1b05-249c-4a1e-ae40-7cb3e42018da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, index_name=\"course-questions\", max_results=5):\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "    \n",
    "    search_query = {\n",
    "        \"size\": max_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body=search_query)\n",
    "    documents = [hit['_source'] for hit in response['hits']['hits']]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab847f6-24cc-485e-aeee-b1dd915b9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "!direnv allow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7788786c-8d56-458c-94c0-b0f6277a73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8084c91-198d-486a-8db4-fa04c35f6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "api_key=os.getenv('GROQ_API_KEY')\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd230ed-80c1-4e4c-86fe-0b2a2c06cf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as accelerated language models or efficient language models, are language models that have been optimized for speed and parallelization, allowing them to process large amounts of text data quickly and efficiently. The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Speed and Efficiency**: Fast language models enable faster processing of large datasets, which is crucial in many applications, such as:\n",
      "\t* Text classification, sentiment analysis, and topic modeling, where speed and efficiency are critical for real-time decision-making.\n",
      "\t* Information retrieval, where rapid processing of queries is essential for providing timely and relevant results.\n",
      "2. **Scalability**: Fast language models can handle larger datasets and more complex computations, making them suitable for:\n",
      "\t* Big data analytics, where processing large datasets quickly is essential for extracting valuable insights.\n",
      "\t* Machine learning, where large datasets require fast processing for training and testing.\n",
      "3. **Parallelization**: Fast language models can be easily parallelized, allowing them to:\n",
      "\t* Take advantage of distributed computing architectures, such as GPU clusters and cloud computing, to accelerate processing.\n",
      "\t* Scale horizontally, effortlessly processing large volumes of data in parallel.\n",
      "4. **Cost-Effectiveness**: Fast language models can reduce computation costs and energy consumption, making them:\n",
      "\t* More cost-effective for large-scale applications, where energy efficiency and cost savings are crucial.\n",
      "5. **Real-time Processing**: Fast language models enable real-time processing of text data, which is critical in applications like:\n",
      "\t* Chatbots, virtual assistants, and content moderation, where immediacy is essential for user experience and satisfaction.\n",
      "\t* Surveillance, law enforcement, and emergency response systems, where timely processing of large volumes of text data is critical for decision-making.\n",
      "6. **Advancements in NLP**: Fast language models drive innovation in Natural Language Processing (NLP) by enabling:\n",
      "\t* Faster experimentation and development of new NLP models and techniques.\n",
      "\t* More effective evaluation of NLP models through faster processing and parallelization.\n",
      "7. **Domain Adaptation**: Fast language models can quickly adapt to new domains, such as:\n",
      "\t* Adapting to new industries or domains, allowing for more accurate and effective processing of text data.\n",
      "\t* Quick incorporation of new language models and techniques, ensuring faster adaptation to emerging trends and technologies.\n",
      "8. **Explainability and Transparency**: Fast language models can provide insights into their processing and decision-making processes, making them:\n",
      "\t* More transparent and explainable, which is essential for building trust in language models and ensuring fair and unbiased decision-making.\n",
      "\n",
      "In summary, fast language models are crucial for modern NLP applications, enabling faster, more efficient, and scalable processing of large text datasets. Their importance lies in their ability to speed up processing, reduce costs, and enable real-time processing, making them essential for various industries and applications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52736f4-a5b6-4bd5-bbc0-5454dfbc8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(documents):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_str = f\"Section: {doc['section']}\\nQuestion: {doc['question']}\\nAnswer: {doc['text']}\\n\\n\"\n",
    "        context += doc_str\n",
    "    \n",
    "    context = context.strip()\n",
    "    return context\n",
    "\n",
    "\n",
    "def build_prompt(user_question, documents):\n",
    "    context = build_context(documents)\n",
    "    return f\"\"\"\n",
    "You're a course teaching assistant.\n",
    "Answer the user QUESTION based on CONTEXT - the documents retrieved from our FAQ database.\n",
    "Don't use other information outside of the provided CONTEXT.  \n",
    "\n",
    "QUESTION: {user_question}\n",
    "\n",
    "CONTEXT:\n",
    "\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "def ask_openai(prompt, model=\"llama3-8b-8192\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def qa_bot(user_question):\n",
    "    context_docs = retrieve_documents(user_question)\n",
    "    prompt = build_prompt(user_question, context_docs)\n",
    "    answer = ask_openai(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fece773-f174-4384-836d-cbdcee8db765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Kafka, you can run the producer/consumer/examples in the terminal using the following command:\\n\\n`java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java`\\n\\nNote that you will need to replace `<jar_name>` with the actual name of your JAR file.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"how can I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d9ef96-392d-40db-a8d4-5a438fb6dae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I see you\\'re getting an \"invalid reference format: repository name must be lowercase\" error!\\n\\nAccording to the context, I\\'d say the problem is likely with the Docker volume mounting. Make sure you\\'re using lowercase letters for the repository name. Try renaming your folder or replacing the \"-v\" part with one of the options provided in the text.\\n\\nFor example, you could try this:\\n\\n`-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data`\\n\\nOr this:\\n\\n`-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data`\\n\\nOr even this:\\n\\n`-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"`\\n\\nRemember to check the quotes\\' position and placement!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"I'm getting invalid reference format: repository name must be lowercase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b48a9e-e026-4bd4-91fd-a399e19bc750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems like you're experiencing an issue with connecting to Postgres port 5432, and the password is not working. \\n\\nIt could be that the port 5432 is taken by another PgSQL server running on your machine. You can try using a different port instead of the default 5432. \\n\\nAlso, if you have a service in Windows running Postgres, stopping that service should resolve the issue.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot(\"I can't connect to postgres port 5432, my password doesn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753ffe8-76d1-4704-baf4-9e8ecfc66f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
